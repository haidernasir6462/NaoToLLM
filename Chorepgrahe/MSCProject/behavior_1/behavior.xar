<?xml version="1.0" encoding="UTF-8" ?>
<ChoregrapheProject xmlns="http://www.ald.softbankrobotics.com/schema/choregraphe/project.xsd" xar_version="3">
  <Box name="root" id="-1" localization="8" tooltip="Root box of Choregraphe&apos;s behavior. Highest level possible." x="0" y="0">
    <bitmap>media/images/box/root.png</bitmap>
    <script language="4">
      <content>
        <![CDATA[]]>
      </content>
    </script>
    <Input name="onLoad" type="1" type_size="1" nature="0" inner="1" tooltip="Signal sent when diagram is loaded." id="1" />
    <Input name="onStart" type="1" type_size="1" nature="2" inner="0" tooltip="Box behavior starts when a signal is received on this input." id="2" />
    <Input name="onStop" type="1" type_size="1" nature="3" inner="0" tooltip="Box behavior stops when a signal is received on this input." id="3" />
    <Output name="onStopped" type="1" type_size="1" nature="1" inner="0" tooltip="Signal sent when box behavior is finished." id="4" />
    <Timeline enable="0">
      <BehaviorLayer name="behavior_layer1">
        <BehaviorKeyframe name="keyframe1" index="1">
          <Diagram scale="100">
            <Box name="LLM_Response" id="2" localization="8" tooltip="Say the text received on its input." x="835" y="1291">
              <bitmap>media/images/box/interaction/say.png</bitmap>
              <script language="4">
                <content>
                  <![CDATA[import requests
from naoqi import ALProxy
import json

class MyClass(GeneratedClass):
    def __init__(self):
        # Correct __init__ method with double underscores
        GeneratedClass.__init__(self)
        self.conversation_history = []  # Initialize the conversation history as a list
        self.full_query = ""  # To store the final query
        self.user_attributes = None  # To store the input from input_1
        self.user_question = None  # To store the input from onStart

    def onLoad(self):
        # Initialization code
        self.tts = ALProxy("ALTextToSpeech")  # Use TTS to speak messages

    def onUnload(self):
        # Clean-up code
        pass

    def onInput_input_1(self, attributes):
        # Store the input from the attributes box
        self.user_attributes = str(attributes)
        self.full_query += " The attributes of the user are " + str(self.user_attributes)

        # Check if both inputs have arrived
        self.process_if_ready()
        self.onStopped()

    def onInput_onStart(self, p):
        # Store the input from the question box
        self.user_question = str(p)
        self.full_query += " The question is: " + self.user_question + ". Give me short and personalized answer based on the attributes of the user and dont mention anything about the attributes in the response"

        # Check if both inputs have arrived
        self.process_if_ready()
        self.onStopped()

    def process_if_ready(self):
        """Process the inputs if both inputs have been received."""
        if self.user_attributes is not None and self.user_question is not None:
            # Both inputs have been received, proceed with sending the request to the Ollama server

            # Add the user's message to the conversation history
            self.conversation_history.append({
                "role": "user",
                "content": self.full_query
            })

            try:
                # Define the URL of the Ollama server API
                url = 'http://192.168.122.153:11434/api/chat'

                # Prepare the payload for Ollama including the conversation history
                payload = {
                    "model": "llama3.1",
                    "messages": self.conversation_history  # Send the conversation history
                }

                self.tts.say("Please wait while I am processing.")
                print("Final query before processing: " + self.full_query)
#                self.tts.say("Final query before processing: " + self.full_query)
#                print(payload)
#                print(self.full_query)

                # Send the request to the Ollama server
                response = requests.post(url, json=payload)

                if response.status_code == 200:
                    full_response = ""
                    # Parse the JSON response
                    raw_data = response.text.strip().splitlines()
                    for item in raw_data:
                        parsed_item = json.loads(item)
                        message = parsed_item.get('message', {})
                        response_value = message.get('content', "")
                        full_response += response_value  # Accumulate the text

                    self.tts.say(str(full_response.strip()))  # Speak the accumulated text

                    self.conversation_history.append({
                        "role": "assistant",
                        "content": full_response.strip()
                    })
                # Activate the output of the box

                else:
                    self.tts.say("Failed to fetch a response. Status code: " + str(response.status_code))

            except Exception as e:
                # Catching any exception and reporting it using TTS
                error_message = "Exception: {0}".format(str(e))
                print(error_message)  # Log the error message for debugging
                self.tts.say(error_message)

            # Activate the output of the box
        self.onStopped()

    def onInput_onStop(self):
        self.onUnload()  # Clean up when the box is stopped
        self.onStopped()  # Activate the output of the box]]>
                </content>
              </script>
              <Input name="onLoad" type="1" type_size="1" nature="0" inner="1" tooltip="Signal sent when Diagram is loaded." id="1" />
              <Input name="onStart" type="0" type_size="1" nature="2" inner="0" tooltip="Box behavior starts when a signal is received on this Input." id="2" />
              <Input name="onStop" type="1" type_size="1" nature="3" inner="0" tooltip="Box behavior stops when a signal is received on this Input." id="3" />
              <Input name="input_1" type="0" type_size="1" nature="1" inner="0" tooltip="" id="4" />
              <Output name="onStopped" type="1" type_size="1" nature="1" inner="0" tooltip="Signal sent when Box behavior is finished." id="5" />
              <Parameter name="Voice shaping (%)" inherits_from_parent="1" content_type="1" value="100" default_value="100" min="50" max="150" tooltip='Used to modify at runtime the voice feature (tone, speed). In a slighty&#x0A;different way than pitch and speed, it gives a kind of &quot;gender or age&#x0A;modification&quot; effect.&#x0A;&#x0A;For instance, a quite good male derivation of female voice can be&#x0A;obtained setting this parameter to 78%.&#x0A;&#x0A;Note: For a better effect, you can compensate this parameter with the&#x0A;speed parameter. For example, if you want to decrease by 20% the voice&#x0A;shaping, you will have to increase by 20% the speed to keep a constant&#x0A;average speed.' id="6" />
              <Parameter name="Speed (%)" inherits_from_parent="1" content_type="1" value="100" default_value="100" min="50" max="200" tooltip="Changes the speed of the voice.&#x0A;&#x0A;Note: For a better effect, you can compensate this parameter with the voice&#x0A;shaping parameter. For example, if you want to increase by 20% the speed, you&#x0A;will have to decrease by 20% the voice shaping to keep a constant average&#x0A;speed." id="7" />
              <Resource name="Speech" type="Lock" timeout="0" />
            </Box>
            <Box name="Listener" id="1" localization="8" tooltip="" x="263" y="1226">
              <bitmap>media/images/box/box-python-script.png</bitmap>
              <script language="4">
                <content>
                  <![CDATA[import os
import time
from naoqi import ALProxy

class MyClass(GeneratedClass):
    def __init__(self):  # Corrected the constructor name
        GeneratedClass.__init__(self)

    def onLoad(self):
        # Initialization code
        self.tts = ALProxy("ALTextToSpeech")
        self.audio_recorder = ALProxy("ALAudioRecorder")
        self.audio_player = ALProxy("ALAudioPlayer")  # Proxy for playing the recorded audio
        self.audio_path = "/home/nao/recordings/microphones/test.wav"  # Path to save the recorded audio

    def onUnload(self):
        # Clean-up code
        pass

    def onInput_onStart(self):
        # Start recording audio
        self.tts.say("I'm listening. Please speak.")
        self.startRecording()

        # Wait for 5 seconds (or you can implement a better method to detect the end of speech)
        time.sleep(5)

        # Stop recording
        self.stopRecording()
        self.tts.say("Recording complete.")


#        # NAO speaks and plays back the recorded audio
#        self.tts.say("I will now play what you said.")
#        self.playRecordedAudio()

        # NAO sends the audio to the next box
        self.sendRecordedAudio()
#        self.tts.say("Recording sent successfully by output 1.")

        # Activate the output of the box
        self.onStopped()

    def startRecording(self):
        """Start the audio recording."""
        channels = [0, 0, 1, 0]
        self.audio_recorder.startMicrophonesRecording(self.audio_path, "wav", 16000, channels)

    def stopRecording(self):
        """Stop the audio recording."""
        self.audio_recorder.stopMicrophonesRecording()


    def sendRecordedAudio(self):
        """Send the recorded audio to the next box."""
        try:
            self.output_1(self.audio_path)  # Send the audio to the next box through output_1
        except Exception as e:
            error_message = "Error sending recorded audio: {0}".format(str(e))
            print(error_message)
            self.tts.say(error_message)

    def onInput_onStop(self):
        self.onUnload()  # Clean up on stop
        self.onStopped()  # Activate the output of the box]]>
                </content>
              </script>
              <Input name="onLoad" type="1" type_size="1" nature="0" inner="1" tooltip="Signal sent when diagram is loaded." id="1" />
              <Input name="onStart" type="1" type_size="1" nature="2" inner="0" tooltip="Box behavior starts when a signal is received on this input." id="2" />
              <Input name="onStop" type="1" type_size="1" nature="3" inner="0" tooltip="Box behavior stops when a signal is received on this input." id="3" />
              <Output name="onStopped" type="1" type_size="1" nature="1" inner="0" tooltip="Signal sent when box behavior is finished." id="4" />
              <Output name="output_1" type="0" type_size="1" nature="1" inner="0" tooltip="" id="5" />
            </Box>
            <Box name="AssemblyAI" id="7" localization="8" tooltip="" x="550" y="1230">
              <bitmap>media/images/box/box-python-script.png</bitmap>
              <script language="4">
                <content>
                  <![CDATA[import time
import requests
from naoqi import ALProxy
import json
import os

class MyClass(GeneratedClass):
    def _init_(self):
        GeneratedClass._init_(self)
        self.audio_file_path = '/home/nao/recordings/microphones/test.wav'  # Update with your audio file path

    def onLoad(self):
        # Initialization code
        self.tts = ALProxy("ALTextToSpeech")  # Use TTS to speak messages
        self.audio_player = ALProxy("ALAudioPlayer")  # Proxy for playing the recorded audio

    def onUnload(self):
        # Clean-up code here
        pass

    def onInput_onStart(self, p ):
#        self.tts.say("I have received the recording. I am going to play it")
##        self.audio_player.playFile(p)  # Play the recorded WAV file

        # Localize the variables to avoid attribute conflicts
        upload_url = "https://api.assemblyai.com/v2/upload"
        transcript_url = "https://api.assemblyai.com/v2/transcript"
        api_key = "6eed597ad7724383be32ae64d9bae503"

        # Step 1: Upload the audio file
        audio_url = self.upload_audio(upload_url, api_key, p)

        if audio_url:
            # Step 2: Request transcription
            transcript_id = self.request_transcription(transcript_url, api_key, audio_url)

            if transcript_id:
                # Step 3: Poll for the transcription status
                transcript = self.poll_transcription_status(transcript_url, api_key, transcript_id)
                if transcript:
                    text = transcript['text']
                    print("Transcription response:", text)
#                    self.tts.say(str(text))  # NAO speaks the transcribed text
                    # NAO sends the audio to the next box

#                    self.tts.say("I am going to send the recording to the next box.")
                    self.SendTranscribedTextToLLama(str(text))
#                    self.tts.say("Recording sent successfully by output 1.")

                else:
                    print("Failed to retrieve the transcription.")
            else:
                print("Failed to get a response from the transcription request.")
        else:
            print("Failed to upload audio.")

        self.onStopped()

    def SendTranscribedTextToLLama(self,Transcribedtext):
        """Send the transcribed text to the next box."""
        try:
            self.output_1(Transcribedtext)  # Send the audio to the next box through output_1
        except Exception as e:
            error_message = "Error sending Transcribed text: {0}".format(str(e))
            print(error_message)
            self.tts.say(error_message)

        pass

    def onInput_onStop(self):
        self.onUnload()  # Clean up when stopped
        self.onStopped()  # Trigger next behavior

    def upload_audio(self, upload_url, api_key, audio_file_path):
        """Upload the audio file to AssemblyAI and return the audio URL."""
        headers = {
            "authorization": api_key,
        }

        with open(audio_file_path, 'rb') as audio_file:
            response = requests.post(upload_url, headers=headers, data=audio_file)

        if response.status_code == 200:
            upload_response = response.json()
            return upload_response['upload_url']  # Return the uploaded audio URL
        else:
            print("Failed to upload audio with status code {}: {}".format(response.status_code, response.text))
            return None

    def request_transcription(self, transcript_url, api_key, audio_url):
        """Send the POST request to AssemblyAI API for transcription and return the transcript ID."""
        headers = {
            "Authorization": api_key,
            "Content-Type": "application/json"
        }
        data = {
            "audio_url": audio_url
        }

        try:
            print("Sending transcription request to AssemblyAI...")
            response = requests.post(transcript_url, headers=headers, json=data)

            if response.status_code == 200:
                transcript_data = response.json()
                return transcript_data['id']  # Return the transcript ID for polling
            else:
                print("Failed with status code {}: {}".format(response.status_code, response.text))
                return None

        except requests.RequestException as e:
            print("An error occurred while sending the request: {}".format(e))
            return None

    def poll_transcription_status(self, transcript_url, api_key, transcript_id):
        """Poll the transcription status until it is completed."""
        headers = {
            "Authorization": api_key,
            "Content-Type": "application/json"
        }

        while True:
            time.sleep(5)  # Wait for 5 seconds before polling
            response = requests.get("{}/{}".format(transcript_url, transcript_id), headers=headers)

            if response.status_code == 200:
                transcript_data = response.json()
                status = transcript_data['status']
                print("Current status:", status)

                if status == 'completed':
                    return transcript_data  # Return the completed transcript data
                elif status == 'failed':
                    print("Transcription failed.")
                    return None
            else:
                print("Failed to poll with status code {}: {}".format(response.status_code, response.text))
                return None]]>
                </content>
              </script>
              <Input name="onLoad" type="1" type_size="1" nature="0" inner="1" tooltip="Signal sent when diagram is loaded." id="1" />
              <Input name="onStart" type="0" type_size="1" nature="2" inner="0" tooltip="Box behavior starts when a signal is received on this input." id="2" />
              <Input name="onStop" type="1" type_size="1" nature="3" inner="0" tooltip="Box behavior stops when a signal is received on this input." id="3" />
              <Output name="onStopped" type="1" type_size="1" nature="1" inner="0" tooltip="Signal sent when box behavior is finished." id="4" />
              <Output name="output_1" type="0" type_size="1" nature="2" inner="0" tooltip="" id="5" />
            </Box>
            <Box name="LLM_Response (1)" id="4" localization="8" tooltip="Say the text received on its input." x="519" y="314">
              <bitmap>media/images/box/interaction/say.png</bitmap>
              <script language="4">
                <content>
                  <![CDATA[import requests
from naoqi import ALProxy
import json

class MyClass(GeneratedClass):
    def __init__(self):
        # Correct __init__ method with double underscores
        GeneratedClass.__init__(self)
        self.conversation_history = []  # Initialize the conversation history as a list

    def onLoad(self):
        # Initialization code
        self.tts = ALProxy("ALTextToSpeech")  # Use TTS to speak messages

    def SendResponseToFilterProducts(self, full_response, user_input):
        """Send the transcribed text to the next box."""
        try:
            self.output_1(full_response)  # Send the resposne to the next box through output_1
#            print(full_response)
#            print(user_input)
            self.output_2(user_input)

        except Exception as e:
            error_message = "Error sending full response: {0}".format(str(e))
            print(error_message)
            self.tts.say(error_message)

    def onUnload(self):
        # Clean-up code
        pass

    def onInput_onStart(self, p):
#        self.tts.say("Starting to send request...")

        user_input = str(p)  # Get the user's input
#        user_input = "test input")  # Get the user's input


        # Add the user's message to the conversation history
        self.conversation_history.append({
            "role": "user",
            "content": user_input + "extract the product name from the given prompt and only say the product name."
        })

        # Example message to demonstrate the working list
#        self.tts.say("User input has been added to history")

        try:
            # Define the URL of the Ollama server API
            url = 'http://192.168.122.153:11434/api/chat'

            # Prepare the payload for Ollama including the conversation history
            payload = {
                "model": "llama3.1",
                "messages": self.conversation_history  # Send the conversation history
            }

            # Send the request to the Ollama server
            response = requests.post(url, json=payload)

            if response.status_code == 200:
                full_response = ""
                # Parse the JSON response
                raw_data = response.text.strip().splitlines()
                for item in raw_data:
                    parsed_item = json.loads(item)
                    message = parsed_item.get('message', {})
                    response_value = message.get('content', "")
                    full_response += response_value  # Accumulate the text

#                self.tts.say(str(full_response.strip()))  # Speak the accumulated text

                self.SendResponseToFilterProducts(str(full_response), user_input)
                self.conversation_history.append({
                    "role": "assistant",
                    "content": full_response.strip()
                })
                # Activate the output of the box


            else:
                self.tts.say("Failed to fetch a response. Status code: " + str(response.status_code))

        except Exception as e:
            # Catching any exception and reporting it using TTS
            error_message = "Exception: {0}".format(str(e))
            print(error_message)  # Log the error message for debugging
            self.tts.say(error_message)


        self.onStopped()

    def onInput_onStop(self, full_query):

        self.onUnload()  # Clean up when the box is stopped
        self.onStopped()  # Activate the output of the box]]>
                </content>
              </script>
              <Input name="onLoad" type="1" type_size="1" nature="0" inner="1" tooltip="Signal sent when Diagram is loaded." id="1" />
              <Input name="onStart" type="3" type_size="1" nature="2" inner="0" tooltip="Box behavior starts when a signal is received on this Input." id="2" />
              <Input name="onStop" type="1" type_size="1" nature="3" inner="0" tooltip="Box behavior stops when a signal is received on this Input." id="3" />
              <Output name="onStopped" type="1" type_size="1" nature="1" inner="0" tooltip="Signal sent when Box behavior is finished." id="4" />
              <Output name="output_1" type="0" type_size="1" nature="2" inner="0" tooltip="" id="5" />
              <Output name="output_2" type="0" type_size="1" nature="2" inner="0" tooltip="" id="6" />
              <Parameter name="Voice shaping (%)" inherits_from_parent="1" content_type="1" value="100" default_value="100" min="50" max="150" tooltip='Used to modify at runtime the voice feature (tone, speed). In a slighty&#x0A;different way than pitch and speed, it gives a kind of &quot;gender or age&#x0A;modification&quot; effect.&#x0A;&#x0A;For instance, a quite good male derivation of female voice can be&#x0A;obtained setting this parameter to 78%.&#x0A;&#x0A;Note: For a better effect, you can compensate this parameter with the&#x0A;speed parameter. For example, if you want to decrease by 20% the voice&#x0A;shaping, you will have to increase by 20% the speed to keep a constant&#x0A;average speed.' id="7" />
              <Parameter name="Speed (%)" inherits_from_parent="1" content_type="1" value="100" default_value="100" min="50" max="200" tooltip="Changes the speed of the voice.&#x0A;&#x0A;Note: For a better effect, you can compensate this parameter with the voice&#x0A;shaping parameter. For example, if you want to increase by 20% the speed, you&#x0A;will have to decrease by 20% the voice shaping to keep a constant average&#x0A;speed." id="8" />
              <Resource name="Speech" type="Lock" timeout="0" />
            </Box>
            <Box name="Listener (1)" id="8" localization="8" tooltip="" x="188" y="126">
              <bitmap>media/images/box/box-python-script.png</bitmap>
              <script language="4">
                <content>
                  <![CDATA[import os
import time
from naoqi import ALProxy

class MyClass(GeneratedClass):
    def __init__(self):  # Corrected the constructor name
        GeneratedClass.__init__(self)

    def onLoad(self):
        # Initialization code
        self.tts = ALProxy("ALTextToSpeech")
        self.audio_recorder = ALProxy("ALAudioRecorder")
        self.audio_player = ALProxy("ALAudioPlayer")  # Proxy for playing the recorded audio
        self.audio_path = "/home/nao/recordings/microphones/test.wav"  # Path to save the recorded audio

    def onUnload(self):
        # Clean-up code
        pass

    def onInput_onStart(self):
        # Start recording audio
        self.tts.say("I'm listening. Please speak.")
        self.startRecording()

        # Wait for 5 seconds (or you can implement a better method to detect the end of speech)
        time.sleep(5)

        # Stop recording
        self.stopRecording()
        self.tts.say("Recording complete.")


#        # NAO speaks and plays back the recorded audio
#        self.tts.say("I will now play what you said.")
#        self.playRecordedAudio()

        # NAO sends the audio to the next box
        self.sendRecordedAudio()
#        self.tts.say("Recording sent successfully by output 1.")

        # Activate the output of the box
        self.onStopped()

    def startRecording(self):
        """Start the audio recording."""
        channels = [0, 0, 1, 0]  # Enable only the front microphone
        self.audio_recorder.startMicrophonesRecording(self.audio_path, "wav", 16000, channels)

    def stopRecording(self):
        """Stop the audio recording."""
        self.audio_recorder.stopMicrophonesRecording()

#    def playRecordedAudio(self):
#        """Play the recorded audio using NAO's speakers."""
#        try:
#            self.audio_player.playFile(self.audio_path)  # Play the recorded WAV file
#        except Exception as e:
#            error_message = "Error playing the recorded audio: {0}".format(str(e))
#            print(error_message)
#            self.tts.say(error_message)

    def sendRecordedAudio(self):
        """Send the recorded audio to the next box."""
        try:
            self.output_1(self.audio_path)  # Send the audio to the next box through output_1
        except Exception as e:
            error_message = "Error sending recorded audio: {0}".format(str(e))
            print(error_message)
            self.tts.say(error_message)

    def onInput_onStop(self):
        self.onUnload()  # Clean up on stop
        self.onStopped()  # Activate the output of the box]]>
                </content>
              </script>
              <Input name="onLoad" type="1" type_size="1" nature="0" inner="1" tooltip="Signal sent when diagram is loaded." id="1" />
              <Input name="onStart" type="1" type_size="1" nature="2" inner="0" tooltip="Box behavior starts when a signal is received on this input." id="2" />
              <Input name="onStop" type="1" type_size="1" nature="3" inner="0" tooltip="Box behavior stops when a signal is received on this input." id="3" />
              <Output name="onStopped" type="1" type_size="1" nature="1" inner="0" tooltip="Signal sent when box behavior is finished." id="4" />
              <Output name="output_1" type="0" type_size="1" nature="1" inner="0" tooltip="" id="5" />
            </Box>
            <Box name="AssemblyAI (1)" id="9" localization="8" tooltip="" x="297" y="332">
              <bitmap>media/images/box/box-python-script.png</bitmap>
              <script language="4">
                <content>
                  <![CDATA[import time
import requests
from naoqi import ALProxy
import json
import os

class MyClass(GeneratedClass):
    def _init_(self):
        GeneratedClass._init_(self)
        self.audio_file_path = '/home/nao/recordings/microphones/test.wav'  # Update with your audio file path

    def onLoad(self):
        # Initialization code
        self.tts = ALProxy("ALTextToSpeech")  # Use TTS to speak messages
        self.audio_player = ALProxy("ALAudioPlayer")  # Proxy for playing the recorded audio

    def onUnload(self):
        # Clean-up code here
        pass

    def onInput_onStart(self, p ):
#        self.tts.say("I have received the recording. I am going to play it")
##        self.audio_player.playFile(p)  # Play the recorded WAV file

        # Localize the variables to avoid attribute conflicts
        upload_url = "https://api.assemblyai.com/v2/upload"
        transcript_url = "https://api.assemblyai.com/v2/transcript"
        api_key = "6eed597ad7724383be32ae64d9bae503"

        # Step 1: Upload the audio file
        audio_url = self.upload_audio(upload_url, api_key, p)

        if audio_url:
            # Step 2: Request transcription
            transcript_id = self.request_transcription(transcript_url, api_key, audio_url)

            if transcript_id:
                # Step 3: Poll for the transcription status
                transcript = self.poll_transcription_status(transcript_url, api_key, transcript_id)
                if transcript:
                    text = transcript['text']
                    print("Transcription response:", text)
#                    self.tts.say(str(text))  # NAO speaks the transcribed text
                    # NAO sends the audio to the next box

#                    self.tts.say("I am going to send the recording to the next box.")
                    self.SendTranscribedTextToLLama(str(text))
#                    self.tts.say("Recording sent successfully by output 1.")

                else:
                    print("Failed to retrieve the transcription.")
            else:
                print("Failed to get a response from the transcription request.")
        else:
            print("Failed to upload audio.")

        self.onStopped()

    def SendTranscribedTextToLLama(self,Transcribedtext):
        """Send the transcribed text to the next box."""
        try:
            self.output_1(Transcribedtext)  # Send the audio to the next box through output_1
        except Exception as e:
            error_message = "Error sending Transcribed text: {0}".format(str(e))
            print(error_message)
            self.tts.say(error_message)

        pass

    def onInput_onStop(self):
        self.onUnload()  # Clean up when stopped
        self.onStopped()  # Trigger next behavior

    def upload_audio(self, upload_url, api_key, audio_file_path):
        """Upload the audio file to AssemblyAI and return the audio URL."""
        headers = {
            "authorization": api_key,
        }

        with open(audio_file_path, 'rb') as audio_file:
            response = requests.post(upload_url, headers=headers, data=audio_file)

        if response.status_code == 200:
            upload_response = response.json()
            return upload_response['upload_url']  # Return the uploaded audio URL
        else:
            print("Failed to upload audio with status code {}: {}".format(response.status_code, response.text))
            return None

    def request_transcription(self, transcript_url, api_key, audio_url):
        """Send the POST request to AssemblyAI API for transcription and return the transcript ID."""
        headers = {
            "Authorization": api_key,
            "Content-Type": "application/json"
        }
        data = {
            "audio_url": audio_url
        }

        try:
            print("Sending transcription request to AssemblyAI...")
            response = requests.post(transcript_url, headers=headers, json=data)

            if response.status_code == 200:
                transcript_data = response.json()
                return transcript_data['id']  # Return the transcript ID for polling
            else:
                print("Failed with status code {}: {}".format(response.status_code, response.text))
                return None

        except requests.RequestException as e:
            print("An error occurred while sending the request: {}".format(e))
            return None

    def poll_transcription_status(self, transcript_url, api_key, transcript_id):
        """Poll the transcription status until it is completed."""
        headers = {
            "Authorization": api_key,
            "Content-Type": "application/json"
        }

        while True:
            time.sleep(5)  # Wait for 5 seconds before polling
            response = requests.get("{}/{}".format(transcript_url, transcript_id), headers=headers)

            if response.status_code == 200:
                transcript_data = response.json()
                status = transcript_data['status']
                print("Current status:", status)

                if status == 'completed':
                    return transcript_data  # Return the completed transcript data
                elif status == 'failed':
                    print("Transcription failed.")
                    return None
            else:
                print("Failed to poll with status code {}: {}".format(response.status_code, response.text))
                return None]]>
                </content>
              </script>
              <Input name="onLoad" type="1" type_size="1" nature="0" inner="1" tooltip="Signal sent when diagram is loaded." id="1" />
              <Input name="onStart" type="0" type_size="1" nature="2" inner="0" tooltip="Box behavior starts when a signal is received on this input." id="2" />
              <Input name="onStop" type="1" type_size="1" nature="3" inner="0" tooltip="Box behavior stops when a signal is received on this input." id="3" />
              <Output name="onStopped" type="1" type_size="1" nature="1" inner="0" tooltip="Signal sent when box behavior is finished." id="4" />
              <Output name="output_1" type="0" type_size="1" nature="2" inner="0" tooltip="" id="5" />
            </Box>
            <Box name="filterProducts" id="3" localization="8" tooltip="" x="693" y="167">
              <bitmap>media/images/box/box-python-script.png</bitmap>
              <script language="4">
                <content>
                  <![CDATA[class MyClass(GeneratedClass):
    def _init_(self):
        GeneratedClass._init_(self)

    def onLoad(self):
        self.tts = ALProxy("ALTextToSpeech")  # Use TTS to speak messages

        # Product database for localization
        self.products_data = [
              {
                "name": "Rice",
                "price": 2.5,
                "ingredients": ["Rice"],
                "allergens": [],
                "location": {"aisle": 7, "section": "Left"}
              },
              {
                "name": "Almond Milk",
                "price": 3.0,
                "ingredients": ["Water", "Almonds"],
                "allergens": ["Nuts"],
                "location": {"aisle": 5, "section": "Right"}
              },
              # Additional product entries...
              {
                "name": "Chickpeas",
                "price": 1.6,
                "ingredients": ["Chickpeas", "Water", "Salt"],
                "allergens": [],
                "location": {"aisle": 8, "section": "Right"}
              }
        ]

    def onUnload(self):
        # Clean-up code here
        pass

    def onInput_onStart(self, product):
        # Example product received
        product = str(product)

        # Fetch location from database
        response = self.find_product_location(product)

        self.output_1(response)

        # Start of execution
        self.onStopped()  # Activate the output of the box

    def onInput_onStop(self):
        self.onUnload()  # Clean-up when the box is stopped
        self.onStopped()  # Activate the output of the box

    def find_product_location(self, product_name):
        # Convert product name to lowercase for case-insensitive matching
        product_name_lower = product_name.lower()

        # Iterate through the products to find a match
        for product in self.products_data:
            if product['name'].lower() == product_name_lower:

                # Extracting product details
                product_name_lower = product['name'].lower()  # Lowercase product name
                aisle = product['location']['aisle']  # Aisle number
                section = product['location']['section']  # Section name
                ingredients = ", ".join(product['ingredients'])  # Joining ingredients into a string
#                price = product['price'])

                # Handle allergens
                if product['allergens']:
                    allergens = ", ".join(product['allergens'])  # Joining allergens into a string
                else:
                    allergens = "No allergens listed"

                # Constructing the return string
                result = (
                    "The product " + product_name_lower +
                    " is located in aisle " + str(aisle) +
                    " and in section " + section +
                    ". Its ingredients are: " + ingredients +
                    ". Allergens: " + allergens + "."
#                    "and price is " + str(price) "."
                )

                return (result)

        return "Product not found."

    def onInput_input_1(self, p):
        # Receive the product name from input
        product_name = str(p)

        # Find the product location
        result = self.find_product_location(product_name)

        # Output the result
        self.tts = ALProxy("ALTextToSpeech")
#        self.tts.say(result)

        self.onStopped()  # Activate the output of the box]]>
                </content>
              </script>
              <Input name="onLoad" type="1" type_size="1" nature="0" inner="1" tooltip="Signal sent when diagram is loaded." id="1" />
              <Input name="onStart" type="0" type_size="1" nature="2" inner="0" tooltip="Box behavior starts when a signal is received on this input." id="2" />
              <Input name="onStop" type="1" type_size="1" nature="3" inner="0" tooltip="Box behavior stops when a signal is received on this input." id="3" />
              <Input name="input_1" type="0" type_size="1" nature="1" inner="0" tooltip="" id="4" />
              <Output name="onStopped" type="1" type_size="1" nature="1" inner="0" tooltip="Signal sent when box behavior is finished." id="5" />
              <Output name="output_1" type="0" type_size="1" nature="2" inner="0" tooltip="" id="6" />
            </Box>
            <Box name="FullQuery" id="5" localization="8" tooltip="" x="847" y="305">
              <bitmap>media/images/box/box-python-script.png</bitmap>
              <script language="4">
                <content>
                  <![CDATA[import requests
from naoqi import ALProxy
import json

class MyClass(GeneratedClass):
    def __init__(self):
        # Correct __init__ method with double underscores
        GeneratedClass.__init__(self)
        self.conversation_history = []  # Initialize the conversation history as a list
        self.query = ""
    def onLoad(self):
        # Initialization code
        self.tts = ALProxy("ALTextToSpeech")  # Use TTS to speak messages

    def onUnload(self):
        # Clean-up code
        pass

    def onInput_input_1(self, full_query):
#        self.tts.say("received query is " + str(full_query))

        # Concatenate to self.query instead of append
        self.query += "customers query is " + str(full_query)
#        self.tts.say(self.query)

        self.onStopped()

    def onInput_onStart(self, p):
        product = str(p)  # Get the user's input
        self.query += " and product detail is " + product

        # Add the user's message to the conversation history
        self.conversation_history.append({
            "role": "user",
            "content": "give me short and concise response of customer query and dont mention anything about the question " + self.query
        })

        # Example message to demonstrate the working list
#        self.tts.say("User input has been added to history")

        try:
            # Define the URL of the Ollama server API
            url = 'http://192.168.122.153:11434/api/chat'

            # Prepare the payload for Ollama including the conversation history
            payload = {
                "model": "llama3.1",
                "messages": self.conversation_history  # Send the conversation history
            }

#            self.tts.say("Final query is" + self.query)
#            self.tts.say("Respond according to question." + self.query)


            # Send the request to the Ollama server
            response = requests.post(url, json=payload)

            if response.status_code == 200:
                full_response = ""
                # Parse the JSON response
                raw_data = response.text.strip().splitlines()
                for item in raw_data:
                    parsed_item = json.loads(item)
                    message = parsed_item.get('message', {})
                    response_value = message.get('content', "")
                    full_response += response_value  # Accumulate the text

                self.tts.say(str(full_response.strip()))  # Speak the accumulated text

                self.conversation_history.append({
                    "role": "assistant",
                    "content": full_response.strip()
                })
                # Activate the output of the box


            else:
                self.tts.say("Failed to fetch a response. Status code: " + str(response.status_code))

        except Exception as e:
            # Catching any exception and reporting it using TTS
            error_message = "Exception: {0}".format(str(e))
            print(error_message)  # Log the error message for debugging
            self.tts.say(error_message)


        self.onStopped()

    def onInput_onStop(self, full_query):

        self.onUnload()  # Clean up when the box is stopped
        self.onStopped()  # Activate the output of the box]]>
                </content>
              </script>
              <Input name="onLoad" type="1" type_size="1" nature="0" inner="1" tooltip="Signal sent when diagram is loaded." id="1" />
              <Input name="onStart" type="0" type_size="1" nature="2" inner="0" tooltip="Box behavior starts when a signal is received on this input." id="2" />
              <Input name="onStop" type="1" type_size="1" nature="3" inner="0" tooltip="Box behavior stops when a signal is received on this input." id="3" />
              <Input name="input_1" type="0" type_size="1" nature="1" inner="0" tooltip="" id="4" />
              <Output name="onStopped" type="1" type_size="1" nature="1" inner="0" tooltip="Signal sent when box behavior is finished." id="5" />
            </Box>
            <Box name="ImageRecognition" id="6" localization="8" tooltip="" x="554" y="1394">
              <bitmap>media/images/box/box-python-script.png</bitmap>
              <script language="4">
                <content>
                  <![CDATA[import requests
from naoqi import ALProxy

class MyClass(GeneratedClass):
    def __init__(self):
        GeneratedClass.__init__(self)
        self.image_path = ""
        self.imgbb_api_key = '74e4ae4b51c99547b60866c0bdba460c'
        self.api_key = 'a3fsh17ltdatvm8k8ibj8b4ubn'
        self.api_secret = '9g88omeu34bqj1jbjgr7eu7ng0'

    def onLoad(self):
        # Put initialization code here
        self.tts = ALProxy("ALTextToSpeech")  # Use TTS to speak messages
        pass

    def onUnload(self):
        # Put clean-up code here
        pass

    def onInput_onStart(self):
#        print(str(p))
#        self.tts.say(str(p))
#        image_path = str(p)
#        self.tts.say("I am in on start")
        try:
            # Upload the image and analyze
            image_url = self.upload_to_imgbb("/home/nao/recordings/cameras/photo.jpg")
            print("image url is " + str(image_url))
            if image_url:
                response = self.analyze_image_skybiometry(str(image_url))
#                print("i am final data" + str(response))
                if response:
                    self.extract_relevant_data(response)
        except Exception as e:
            print("Error in onInput_onStart in image Recognition box:", str(e))

    def onInput_onStop(self):
        """Stop the process and clean up resources."""
        self.onUnload()  # Reuse clean-up as the box is stopped
        self.onStopped()  # Activate the output of the box

    def upload_to_imgbb(self, image_path):
        try:
            with open(image_path, 'rb') as image_file:
                url = "https://api.imgbb.com/1/upload"
                payload = {
                    'key': self.imgbb_api_key,
                }
                files = {
                    'image': image_file,  # Correct variable name here
                }
                response = requests.post(url, data=payload, files=files)
                result = response.json()
                return result['data']['url']

#                if response.status_code == 200:
#                    result = response.json()
#                    return result['data']['url']
#                else:
#                    raise Exception("Failed to upload to ImgBB: " + response.text)
        except Exception as e:
            print("Error in upload_to_imgbb:" + e)

    def analyze_image_skybiometry(self, image_url):
        """Analyze the image URL using SkyBiometry API."""
        try:
            url = 'https://api.skybiometry.com/fc/faces/detect.json'
            params = {
                'api_key': self.api_key,  # Use self.api_key
                'api_secret': self.api_secret,  # Use self.api_secret
                'urls': image_url,
                'attributes': 'all',  # Get emotions, age, gender, etc.
            }
            response = requests.get(url, params=params)
            if response.status_code == 200:
                return response.json()
            else:
                raise Exception("Failed to analyze image with SkyBiometry: " + response.text)
        except Exception as e:
            print("Error in analyze_image_skybiometry:" + e)

    def extract_relevant_data(self, response):
        try:
            # Confirm the response has the expected structure

            # Safely extract the 'photos' and 'tags' fields
            photos = response.get('photos', [])
            if not photos:
                raise ValueError("No photos found in the response")

            tags = photos[0].get('tags', [])
            if not tags:
                raise ValueError("No tags found for the photo")

            # Extract the first face data from the tags
            face = tags[0]

            # Extract relevant attributes safely
            attributes = face.get('attributes', {})
            gender = attributes.get('gender', {}).get('value', 'Unknown')
            age_est = attributes.get('age_est', {}).get('value', 'Unknown')
            mood = attributes.get('mood', {}).get('value', 'Unknown')
            liveness = attributes.get('liveness', {}).get('value', 'Unknown')

#            self.tts.say("Information extracted successfully")


            # Check if all required attributes are present
            if gender != 'Unknown' and age_est != 'Unknown' and mood != 'Unknown' and liveness != 'Unknown':
                # Create the final attributes string
                attributes_str = (
                    "the gender is " + str(gender) + ", estimated age is " + str(age_est) + ", the mood is " + str(mood) +               ", and the liveliness is " + str(liveness) + "."
                )

                print(attributes_str)
                self.output_1(attributes_str)
            else:
                # Handle the case where some attributes are missing
                self.tts.say("Missing some attributes in the picture.")
                print("Missing attributes in the picture: Gender: {}, Age: {}, Mood: {}, Liveness: {}".format(
                    gender, age_est, mood, liveness
                ))
                self.output_1("None")

        except Exception as e:
            # Catch and log any errors
            error_message = "Error extracting relevant data: " + str(e)
            print(error_message)
            self.tts.say(error_message)


    def onInput_onStop(self):
        """Stop the process and clean up resources."""
        self.onUnload()  # Reuse clean-up as the box is stopped
        self.onStopped()  # Activate the output of the box]]>
                </content>
              </script>
              <Input name="onLoad" type="1" type_size="1" nature="0" inner="1" tooltip="Signal sent when diagram is loaded." id="1" />
              <Input name="onStart" type="0" type_size="1" nature="2" inner="0" tooltip="Box behavior starts when a signal is received on this input." id="2" />
              <Input name="onStop" type="1" type_size="1" nature="3" inner="0" tooltip="Box behavior stops when a signal is received on this input." id="3" />
              <Output name="onStopped" type="1" type_size="1" nature="1" inner="0" tooltip="Signal sent when box behavior is finished." id="4" />
              <Output name="output_1" type="0" type_size="1" nature="2" inner="0" tooltip="" id="5" />
            </Box>
            <Box name="Capturer" id="10" localization="8" tooltip="" x="240" y="1397">
              <bitmap>media/images/box/box-python-script.png</bitmap>
              <script language="4">
                <content>
                  <![CDATA[from naoqi import ALProxy
import time

class MyClass(GeneratedClass):
    def __init__(self):
        GeneratedClass.__init__(self)
        # Initialize proxies in the constructor
        self.tts = None
        self.photoCaptureProxy = None

    def onLoad(self):
        # Initialization code here
        self.tts = ALProxy("ALTextToSpeech", "127.0.0.1", 9559)
        self.photoCaptureProxy = ALProxy("ALPhotoCapture", "127.0.0.1", 9559)

    def onUnload(self):
        # Clean-up code here
        pass

    def onInput_onStart(self):
        # When this method is triggered, the robot will take a photo and speak
        image_path = self.take_photo_and_speak()

        # Send the image path to the next box
        self.output_image_path(image_path)

        self.onStopped()  # Signal that the box's action has finished

    def onInput_onStop(self):
        self.onUnload()  # It is recommended to reuse the clean-up as the box is stopped
        self.onStopped()  # Activate the output of the box

    def take_photo_and_speak(self):
        try:

            time.sleep(2)

            # Set camera parameters
            self.photoCaptureProxy.setResolution(2)  # 2 is 640x480 resolution
            self.photoCaptureProxy.setPictureFormat("jpg")  # Save format in jpg

            # Define the folder and filename
            folder_path = "/home/nao/recordings/cameras/"
            file_name = "photo"

            # Take the photo and save it
            self.photoCaptureProxy.takePicture(folder_path, file_name)

            # The complete path of the saved image
            image_path = folder_path + file_name + ".jpg"

            # Wait for a moment to ensure everything is saved
            time.sleep(1)

            # Robot speaks the path of the saved photo
            # Return the image path
            return image_path

        except Exception as e:
            # Print the error if any occurs
            print("Error while taking photo: ", str(e))
            return None

    def output_image_path(self, image_path):
        if image_path:
            # This method sends the image path to the next connected box
            self.output_1(image_path)]]>
                </content>
              </script>
              <Input name="onLoad" type="1" type_size="1" nature="0" inner="1" tooltip="Signal sent when diagram is loaded." id="1" />
              <Input name="onStart" type="1" type_size="1" nature="2" inner="0" tooltip="Box behavior starts when a signal is received on this input." id="2" />
              <Input name="onStop" type="1" type_size="1" nature="3" inner="0" tooltip="Box behavior stops when a signal is received on this input." id="3" />
              <Output name="onStopped" type="1" type_size="1" nature="1" inner="0" tooltip="Signal sent when box behavior is finished." id="4" />
              <Output name="output_1" type="0" type_size="1" nature="2" inner="0" tooltip="" id="5" />
            </Box>
            <Link inputowner="7" indexofinput="2" outputowner="1" indexofoutput="5" />
            <Link inputowner="2" indexofinput="2" outputowner="7" indexofoutput="5" />
            <Link inputowner="9" indexofinput="2" outputowner="8" indexofoutput="5" />
            <Link inputowner="4" indexofinput="2" outputowner="9" indexofoutput="5" />
            <Link inputowner="5" indexofinput="4" outputowner="4" indexofoutput="6" />
            <Link inputowner="5" indexofinput="2" outputowner="3" indexofoutput="6" />
            <Link inputowner="3" indexofinput="2" outputowner="4" indexofoutput="5" />
            <Link inputowner="6" indexofinput="2" outputowner="10" indexofoutput="5" />
            <Link inputowner="2" indexofinput="4" outputowner="6" indexofoutput="5" />
            <Link inputowner="1" indexofinput="2" outputowner="0" indexofoutput="2" />
            <Link inputowner="10" indexofinput="2" outputowner="0" indexofoutput="2" />
          </Diagram>
        </BehaviorKeyframe>
      </BehaviorLayer>
    </Timeline>
  </Box>
</ChoregrapheProject>
