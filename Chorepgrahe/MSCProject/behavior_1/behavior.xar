<?xml version="1.0" encoding="UTF-8" ?>
<ChoregrapheProject xmlns="http://www.ald.softbankrobotics.com/schema/choregraphe/project.xsd" xar_version="3">
  <Box name="root" id="-1" localization="8" tooltip="Root box of Choregraphe&apos;s behavior. Highest level possible." x="0" y="0">
    <bitmap>media/images/box/root.png</bitmap>
    <script language="4">
      <content>
        <![CDATA[]]>
      </content>
    </script>
    <Input name="onLoad" type="1" type_size="1" nature="0" inner="1" tooltip="Signal sent when diagram is loaded." id="1" />
    <Input name="onStart" type="1" type_size="1" nature="2" inner="0" tooltip="Box behavior starts when a signal is received on this input." id="2" />
    <Input name="onStop" type="1" type_size="1" nature="3" inner="0" tooltip="Box behavior stops when a signal is received on this input." id="3" />
    <Output name="onStopped" type="1" type_size="1" nature="1" inner="0" tooltip="Signal sent when box behavior is finished." id="4" />
    <Timeline enable="0">
      <BehaviorLayer name="behavior_layer1">
        <BehaviorKeyframe name="keyframe1" index="1">
          <Diagram scale="100">
            <Box name="LLM_Response" id="2" localization="8" tooltip="Say the text received on its input." x="630" y="86">
              <bitmap>media/images/box/interaction/say.png</bitmap>
              <script language="4">
                <content>
                  <![CDATA[import requests
from naoqi import ALProxy
import json

class MyClass(GeneratedClass):
    def __init__(self):
        # Correct __init__ method with double underscores
        GeneratedClass.__init__(self)
        self.conversation_history = []  # Initialize the conversation history as a list

    def onLoad(self):
        # Initialization code
        self.tts = ALProxy("ALTextToSpeech")  # Use TTS to speak messages

    def onUnload(self):
        # Clean-up code
        pass

    def onInput_onStart(self, p):
#        self.tts.say("Starting to send request...")

        user_input = str(p)  # Get the user's input

        # Add the user's message to the conversation history
        self.conversation_history.append({
            "role": "user",
            "content": user_input + "give just a few lines answer"
        })

        # Example message to demonstrate the working list
#        self.tts.say("User input has been added to history")

        try:
            # Define the URL of the Ollama server API
            url = 'http://192.168.76.153:11434/api/chat'

            # Prepare the payload for Ollama including the conversation history
            payload = {
                "model": "llama3.1",
                "messages": self.conversation_history  # Send the conversation history
            }

            self.tts.say("Please wait while i am processing.")

            # Send the request to the Ollama server
            response = requests.post(url, json=payload)

            if response.status_code == 200:
                full_response = ""
                self.tts.say("received response")

                # Parse the JSON response
                raw_data = response.text.strip().splitlines()
                for item in raw_data:
                    parsed_item = json.loads(item)
                    message = parsed_item.get('message', {})
                    response_value = message.get('content', "")
                    full_response += response_value + " "  # Accumulate the text

                self.tts.say(str(full_response.strip()))  # Speak the accumulated text
                self.conversation_history.append({
                    "role": "assistant",
                    "content": full_response.strip()
                })


            else:
                self.tts.say("Failed to fetch a response. Status code: " + str(response.status_code))

        except Exception as e:
            # Catching any exception and reporting it using TTS
            error_message = "Exception: {0}".format(str(e))
            print(error_message)  # Log the error message for debugging
            self.tts.say(error_message)

        # Activate the output of the box
        self.onStopped()

    def onInput_onStop(self):
        self.onUnload()  # Clean up when the box is stopped
        self.onStopped()  # Activate the output of the box]]>
                </content>
              </script>
              <Input name="onLoad" type="1" type_size="1" nature="0" inner="1" tooltip="Signal sent when Diagram is loaded." id="1" />
              <Input name="onStart" type="3" type_size="1" nature="2" inner="0" tooltip="Box behavior starts when a signal is received on this Input." id="2" />
              <Input name="onStop" type="1" type_size="1" nature="3" inner="0" tooltip="Box behavior stops when a signal is received on this Input." id="3" />
              <Output name="onStopped" type="1" type_size="1" nature="1" inner="0" tooltip="Signal sent when Box behavior is finished." id="4" />
              <Parameter name="Voice shaping (%)" inherits_from_parent="1" content_type="1" value="100" default_value="100" min="50" max="150" tooltip='Used to modify at runtime the voice feature (tone, speed). In a slighty&#x0A;different way than pitch and speed, it gives a kind of &quot;gender or age&#x0A;modification&quot; effect.&#x0A;&#x0A;For instance, a quite good male derivation of female voice can be&#x0A;obtained setting this parameter to 78%.&#x0A;&#x0A;Note: For a better effect, you can compensate this parameter with the&#x0A;speed parameter. For example, if you want to decrease by 20% the voice&#x0A;shaping, you will have to increase by 20% the speed to keep a constant&#x0A;average speed.' id="5" />
              <Parameter name="Speed (%)" inherits_from_parent="1" content_type="1" value="100" default_value="100" min="50" max="200" tooltip="Changes the speed of the voice.&#x0A;&#x0A;Note: For a better effect, you can compensate this parameter with the voice&#x0A;shaping parameter. For example, if you want to increase by 20% the speed, you&#x0A;will have to decrease by 20% the voice shaping to keep a constant average&#x0A;speed." id="6" />
              <Resource name="Speech" type="Lock" timeout="0" />
            </Box>
            <Box name="Listener" id="1" localization="8" tooltip="" x="382" y="81">
              <bitmap>media/images/box/box-python-script.png</bitmap>
              <script language="4">
                <content>
                  <![CDATA[import os
import time
from naoqi import ALProxy

class MyClass(GeneratedClass):
    def __init__(self):  # Corrected the constructor name
        GeneratedClass.__init__(self)

    def onLoad(self):
        # Initialization code
        self.tts = ALProxy("ALTextToSpeech")
        self.audio_recorder = ALProxy("ALAudioRecorder")
        self.audio_player = ALProxy("ALAudioPlayer")  # Proxy for playing the recorded audio
        self.audio_path = "/home/nao/recordings/microphones/test.wav"  # Path to save the recorded audio

    def onUnload(self):
        # Clean-up code
        pass

    def onInput_onStart(self):
        # Start recording audio
        self.tts.say("I'm listening. Please speak.")
        self.startRecording()

        # Wait for 5 seconds (or you can implement a better method to detect the end of speech)
        time.sleep(5)

        # Stop recording
        self.stopRecording()
        self.tts.say("Recording complete.")


#        # NAO speaks and plays back the recorded audio
#        self.tts.say("I will now play what you said.")
#        self.playRecordedAudio()

        # NAO sends the audio to the next box
        self.sendRecordedAudio()
#        self.tts.say("Recording sent successfully by output 1.")

        # Activate the output of the box
        self.onStopped()

    def startRecording(self):
        """Start the audio recording."""
        channels = [0, 0, 1, 0]  # Enable only the front microphone
        self.audio_recorder.startMicrophonesRecording(self.audio_path, "wav", 16000, channels)

    def stopRecording(self):
        """Stop the audio recording."""
        self.audio_recorder.stopMicrophonesRecording()

#    def playRecordedAudio(self):
#        """Play the recorded audio using NAO's speakers."""
#        try:
#            self.audio_player.playFile(self.audio_path)  # Play the recorded WAV file
#        except Exception as e:
#            error_message = "Error playing the recorded audio: {0}".format(str(e))
#            print(error_message)
#            self.tts.say(error_message)

    def sendRecordedAudio(self):
        """Send the recorded audio to the next box."""
        try:
            self.output_1(self.audio_path)  # Send the audio to the next box through output_1
        except Exception as e:
            error_message = "Error sending recorded audio: {0}".format(str(e))
            print(error_message)
            self.tts.say(error_message)

    def onInput_onStop(self):
        self.onUnload()  # Clean up on stop
        self.onStopped()  # Activate the output of the box]]>
                </content>
              </script>
              <Input name="onLoad" type="1" type_size="1" nature="0" inner="1" tooltip="Signal sent when diagram is loaded." id="1" />
              <Input name="onStart" type="1" type_size="1" nature="2" inner="0" tooltip="Box behavior starts when a signal is received on this input." id="2" />
              <Input name="onStop" type="1" type_size="1" nature="3" inner="0" tooltip="Box behavior stops when a signal is received on this input." id="3" />
              <Output name="onStopped" type="1" type_size="1" nature="1" inner="0" tooltip="Signal sent when box behavior is finished." id="4" />
              <Output name="output_1" type="0" type_size="1" nature="1" inner="0" tooltip="" id="5" />
            </Box>
            <Box name="AssemblyAI" id="7" localization="8" tooltip="" x="514" y="85">
              <bitmap>media/images/box/box-python-script.png</bitmap>
              <script language="4">
                <content>
                  <![CDATA[import time
import requests
from naoqi import ALProxy
import json
import os

class MyClass(GeneratedClass):
    def _init_(self):
        GeneratedClass._init_(self)
        self.audio_file_path = '/home/nao/recordings/microphones/test.wav'  # Update with your audio file path

    def onLoad(self):
        # Initialization code
        self.tts = ALProxy("ALTextToSpeech")  # Use TTS to speak messages
        self.audio_player = ALProxy("ALAudioPlayer")  # Proxy for playing the recorded audio

    def onUnload(self):
        # Clean-up code here
        pass

    def onInput_onStart(self, p ):
#        self.tts.say("I have received the recording. I am going to play it")
##        self.audio_player.playFile(p)  # Play the recorded WAV file

        # Localize the variables to avoid attribute conflicts
        upload_url = "https://api.assemblyai.com/v2/upload"
        transcript_url = "https://api.assemblyai.com/v2/transcript"
        api_key = "6eed597ad7724383be32ae64d9bae503"

        # Step 1: Upload the audio file
        audio_url = self.upload_audio(upload_url, api_key, p)

        if audio_url:
            # Step 2: Request transcription
            transcript_id = self.request_transcription(transcript_url, api_key, audio_url)

            if transcript_id:
                # Step 3: Poll for the transcription status
                transcript = self.poll_transcription_status(transcript_url, api_key, transcript_id)
                if transcript:
                    text = transcript['text']
                    print("Transcription response:", text)
#                    self.tts.say(str(text))  # NAO speaks the transcribed text
                    # NAO sends the audio to the next box

#                    self.tts.say("I am going to send the recording to the next box.")
                    self.SendTranscribedTextToLLama(str(text))
#                    self.tts.say("Recording sent successfully by output 1.")

                else:
                    print("Failed to retrieve the transcription.")
            else:
                print("Failed to get a response from the transcription request.")
        else:
            print("Failed to upload audio.")

        self.onStopped()

    def SendTranscribedTextToLLama(self,Transcribedtext):
        """Send the transcribed text to the next box."""
        try:
            self.output_1(Transcribedtext)  # Send the audio to the next box through output_1
        except Exception as e:
            error_message = "Error sending Transcribed text: {0}".format(str(e))
            print(error_message)
            self.tts.say(error_message)

        pass

    def onInput_onStop(self):
        self.onUnload()  # Clean up when stopped
        self.onStopped()  # Trigger next behavior

    def upload_audio(self, upload_url, api_key, audio_file_path):
        """Upload the audio file to AssemblyAI and return the audio URL."""
        headers = {
            "authorization": api_key,
        }

        with open(audio_file_path, 'rb') as audio_file:
            response = requests.post(upload_url, headers=headers, data=audio_file)

        if response.status_code == 200:
            upload_response = response.json()
            return upload_response['upload_url']  # Return the uploaded audio URL
        else:
            print("Failed to upload audio with status code {}: {}".format(response.status_code, response.text))
            return None

    def request_transcription(self, transcript_url, api_key, audio_url):
        """Send the POST request to AssemblyAI API for transcription and return the transcript ID."""
        headers = {
            "Authorization": api_key,
            "Content-Type": "application/json"
        }
        data = {
            "audio_url": audio_url
        }

        try:
            print("Sending transcription request to AssemblyAI...")
            response = requests.post(transcript_url, headers=headers, json=data)

            if response.status_code == 200:
                transcript_data = response.json()
                return transcript_data['id']  # Return the transcript ID for polling
            else:
                print("Failed with status code {}: {}".format(response.status_code, response.text))
                return None

        except requests.RequestException as e:
            print("An error occurred while sending the request: {}".format(e))
            return None

    def poll_transcription_status(self, transcript_url, api_key, transcript_id):
        """Poll the transcription status until it is completed."""
        headers = {
            "Authorization": api_key,
            "Content-Type": "application/json"
        }

        while True:
            time.sleep(5)  # Wait for 5 seconds before polling
            response = requests.get("{}/{}".format(transcript_url, transcript_id), headers=headers)

            if response.status_code == 200:
                transcript_data = response.json()
                status = transcript_data['status']
                print("Current status:", status)

                if status == 'completed':
                    return transcript_data  # Return the completed transcript data
                elif status == 'failed':
                    print("Transcription failed.")
                    return None
            else:
                print("Failed to poll with status code {}: {}".format(response.status_code, response.text))
                return None]]>
                </content>
              </script>
              <Input name="onLoad" type="1" type_size="1" nature="0" inner="1" tooltip="Signal sent when diagram is loaded." id="1" />
              <Input name="onStart" type="0" type_size="1" nature="2" inner="0" tooltip="Box behavior starts when a signal is received on this input." id="2" />
              <Input name="onStop" type="1" type_size="1" nature="3" inner="0" tooltip="Box behavior stops when a signal is received on this input." id="3" />
              <Output name="onStopped" type="1" type_size="1" nature="1" inner="0" tooltip="Signal sent when box behavior is finished." id="4" />
              <Output name="output_1" type="0" type_size="1" nature="2" inner="0" tooltip="" id="5" />
            </Box>
            <Box name="LLM_Response (1)" id="4" localization="8" tooltip="Say the text received on its input." x="318" y="443">
              <bitmap>media/images/box/interaction/say.png</bitmap>
              <script language="4">
                <content>
                  <![CDATA[import requests
from naoqi import ALProxy
import json

class MyClass(GeneratedClass):
    def __init__(self):
        # Correct __init__ method with double underscores
        GeneratedClass.__init__(self)
        self.conversation_history = []  # Initialize the conversation history as a list

    def onLoad(self):
        # Initialization code
        self.tts = ALProxy("ALTextToSpeech")  # Use TTS to speak messages

    def SendResponseToFilterProducts(self, full_response, user_input):
        """Send the transcribed text to the next box."""
        try:
            self.output_1(full_response)  # Send the resposne to the next box through output_1
            self.tts.say("successfully sent through output 1")
            self.output_2(user_input)
            self.tts.say("successfully sent through output 2")

        except Exception as e:
            error_message = "Error sending full response: {0}".format(str(e))
            print(error_message)
            self.tts.say(error_message)

    def onUnload(self):
        # Clean-up code
        pass

    def onInput_onStart(self, p):
#        self.tts.say("Starting to send request...")

        user_input = str(p)  # Get the user's input
#        user_input = "test input")  # Get the user's input


        # Add the user's message to the conversation history
        self.conversation_history.append({
            "role": "user",
            "content": user_input + "extract the product name from the given prompt and only say the product name. if there is no product found, just simply say no product found."
        })

        # Example message to demonstrate the working list
#        self.tts.say("User input has been added to history")

        try:
            # Define the URL of the Ollama server API
            url = 'http://192.168.76.153:11434/api/chat'

            # Prepare the payload for Ollama including the conversation history
            payload = {
                "model": "llama3.1",
                "messages": self.conversation_history  # Send the conversation history
            }

            self.tts.say("Please wait while i am processing.")

            # Send the request to the Ollama server
            response = requests.post(url, json=payload)

            if response.status_code == 200:
                full_response = ""
                # Parse the JSON response
                raw_data = response.text.strip().splitlines()
                for item in raw_data:
                    parsed_item = json.loads(item)
                    message = parsed_item.get('message', {})
                    response_value = message.get('content', "")
                    full_response += response_value  # Accumulate the text

                self.tts.say(str(full_response.strip()))  # Speak the accumulated text

                self.SendResponseToFilterProducts(str(full_response), user_input)
                self.conversation_history.append({
                    "role": "assistant",
                    "content": full_response.strip()
                })
                # Activate the output of the box


            else:
                self.tts.say("Failed to fetch a response. Status code: " + str(response.status_code))

        except Exception as e:
            # Catching any exception and reporting it using TTS
            error_message = "Exception: {0}".format(str(e))
            print(error_message)  # Log the error message for debugging
            self.tts.say(error_message)


        self.onStopped()

    def onInput_onStop(self, full_query):

        self.onUnload()  # Clean up when the box is stopped
        self.onStopped()  # Activate the output of the box]]>
                </content>
              </script>
              <Input name="onLoad" type="1" type_size="1" nature="0" inner="1" tooltip="Signal sent when Diagram is loaded." id="1" />
              <Input name="onStart" type="3" type_size="1" nature="2" inner="0" tooltip="Box behavior starts when a signal is received on this Input." id="2" />
              <Input name="onStop" type="1" type_size="1" nature="3" inner="0" tooltip="Box behavior stops when a signal is received on this Input." id="3" />
              <Output name="onStopped" type="1" type_size="1" nature="1" inner="0" tooltip="Signal sent when Box behavior is finished." id="4" />
              <Output name="output_1" type="0" type_size="1" nature="2" inner="0" tooltip="" id="5" />
              <Output name="output_2" type="0" type_size="1" nature="2" inner="0" tooltip="" id="6" />
              <Parameter name="Voice shaping (%)" inherits_from_parent="1" content_type="1" value="100" default_value="100" min="50" max="150" tooltip='Used to modify at runtime the voice feature (tone, speed). In a slighty&#x0A;different way than pitch and speed, it gives a kind of &quot;gender or age&#x0A;modification&quot; effect.&#x0A;&#x0A;For instance, a quite good male derivation of female voice can be&#x0A;obtained setting this parameter to 78%.&#x0A;&#x0A;Note: For a better effect, you can compensate this parameter with the&#x0A;speed parameter. For example, if you want to decrease by 20% the voice&#x0A;shaping, you will have to increase by 20% the speed to keep a constant&#x0A;average speed.' id="7" />
              <Parameter name="Speed (%)" inherits_from_parent="1" content_type="1" value="100" default_value="100" min="50" max="200" tooltip="Changes the speed of the voice.&#x0A;&#x0A;Note: For a better effect, you can compensate this parameter with the voice&#x0A;shaping parameter. For example, if you want to increase by 20% the speed, you&#x0A;will have to decrease by 20% the voice shaping to keep a constant average&#x0A;speed." id="8" />
              <Resource name="Speech" type="Lock" timeout="0" />
            </Box>
            <Box name="Listener (1)" id="8" localization="8" tooltip="" x="92" y="428">
              <bitmap>media/images/box/box-python-script.png</bitmap>
              <script language="4">
                <content>
                  <![CDATA[import os
import time
from naoqi import ALProxy

class MyClass(GeneratedClass):
    def __init__(self):  # Corrected the constructor name
        GeneratedClass.__init__(self)

    def onLoad(self):
        # Initialization code
        self.tts = ALProxy("ALTextToSpeech")
        self.audio_recorder = ALProxy("ALAudioRecorder")
        self.audio_player = ALProxy("ALAudioPlayer")  # Proxy for playing the recorded audio
        self.audio_path = "/home/nao/recordings/microphones/test.wav"  # Path to save the recorded audio

    def onUnload(self):
        # Clean-up code
        pass

    def onInput_onStart(self):
        # Start recording audio
        self.tts.say("I'm listening. Please speak.")
        self.startRecording()

        # Wait for 5 seconds (or you can implement a better method to detect the end of speech)
        time.sleep(5)

        # Stop recording
        self.stopRecording()
        self.tts.say("Recording complete.")


#        # NAO speaks and plays back the recorded audio
#        self.tts.say("I will now play what you said.")
#        self.playRecordedAudio()

        # NAO sends the audio to the next box
        self.sendRecordedAudio()
#        self.tts.say("Recording sent successfully by output 1.")

        # Activate the output of the box
        self.onStopped()

    def startRecording(self):
        """Start the audio recording."""
        channels = [0, 0, 1, 0]  # Enable only the front microphone
        self.audio_recorder.startMicrophonesRecording(self.audio_path, "wav", 16000, channels)

    def stopRecording(self):
        """Stop the audio recording."""
        self.audio_recorder.stopMicrophonesRecording()

#    def playRecordedAudio(self):
#        """Play the recorded audio using NAO's speakers."""
#        try:
#            self.audio_player.playFile(self.audio_path)  # Play the recorded WAV file
#        except Exception as e:
#            error_message = "Error playing the recorded audio: {0}".format(str(e))
#            print(error_message)
#            self.tts.say(error_message)

    def sendRecordedAudio(self):
        """Send the recorded audio to the next box."""
        try:
            self.output_1(self.audio_path)  # Send the audio to the next box through output_1
        except Exception as e:
            error_message = "Error sending recorded audio: {0}".format(str(e))
            print(error_message)
            self.tts.say(error_message)

    def onInput_onStop(self):
        self.onUnload()  # Clean up on stop
        self.onStopped()  # Activate the output of the box]]>
                </content>
              </script>
              <Input name="onLoad" type="1" type_size="1" nature="0" inner="1" tooltip="Signal sent when diagram is loaded." id="1" />
              <Input name="onStart" type="1" type_size="1" nature="2" inner="0" tooltip="Box behavior starts when a signal is received on this input." id="2" />
              <Input name="onStop" type="1" type_size="1" nature="3" inner="0" tooltip="Box behavior stops when a signal is received on this input." id="3" />
              <Output name="onStopped" type="1" type_size="1" nature="1" inner="0" tooltip="Signal sent when box behavior is finished." id="4" />
              <Output name="output_1" type="0" type_size="1" nature="1" inner="0" tooltip="" id="5" />
            </Box>
            <Box name="AssemblyAI (1)" id="9" localization="8" tooltip="" x="208" y="431">
              <bitmap>media/images/box/box-python-script.png</bitmap>
              <script language="4">
                <content>
                  <![CDATA[import time
import requests
from naoqi import ALProxy
import json
import os

class MyClass(GeneratedClass):
    def _init_(self):
        GeneratedClass._init_(self)
        self.audio_file_path = '/home/nao/recordings/microphones/test.wav'  # Update with your audio file path

    def onLoad(self):
        # Initialization code
        self.tts = ALProxy("ALTextToSpeech")  # Use TTS to speak messages
        self.audio_player = ALProxy("ALAudioPlayer")  # Proxy for playing the recorded audio

    def onUnload(self):
        # Clean-up code here
        pass

    def onInput_onStart(self, p ):
#        self.tts.say("I have received the recording. I am going to play it")
##        self.audio_player.playFile(p)  # Play the recorded WAV file

        # Localize the variables to avoid attribute conflicts
        upload_url = "https://api.assemblyai.com/v2/upload"
        transcript_url = "https://api.assemblyai.com/v2/transcript"
        api_key = "6eed597ad7724383be32ae64d9bae503"

        # Step 1: Upload the audio file
        audio_url = self.upload_audio(upload_url, api_key, p)

        if audio_url:
            # Step 2: Request transcription
            transcript_id = self.request_transcription(transcript_url, api_key, audio_url)

            if transcript_id:
                # Step 3: Poll for the transcription status
                transcript = self.poll_transcription_status(transcript_url, api_key, transcript_id)
                if transcript:
                    text = transcript['text']
                    print("Transcription response:", text)
#                    self.tts.say(str(text))  # NAO speaks the transcribed text
                    # NAO sends the audio to the next box

#                    self.tts.say("I am going to send the recording to the next box.")
                    self.SendTranscribedTextToLLama(str(text))
#                    self.tts.say("Recording sent successfully by output 1.")

                else:
                    print("Failed to retrieve the transcription.")
            else:
                print("Failed to get a response from the transcription request.")
        else:
            print("Failed to upload audio.")

        self.onStopped()

    def SendTranscribedTextToLLama(self,Transcribedtext):
        """Send the transcribed text to the next box."""
        try:
            self.output_1(Transcribedtext)  # Send the audio to the next box through output_1
        except Exception as e:
            error_message = "Error sending Transcribed text: {0}".format(str(e))
            print(error_message)
            self.tts.say(error_message)

        pass

    def onInput_onStop(self):
        self.onUnload()  # Clean up when stopped
        self.onStopped()  # Trigger next behavior

    def upload_audio(self, upload_url, api_key, audio_file_path):
        """Upload the audio file to AssemblyAI and return the audio URL."""
        headers = {
            "authorization": api_key,
        }

        with open(audio_file_path, 'rb') as audio_file:
            response = requests.post(upload_url, headers=headers, data=audio_file)

        if response.status_code == 200:
            upload_response = response.json()
            return upload_response['upload_url']  # Return the uploaded audio URL
        else:
            print("Failed to upload audio with status code {}: {}".format(response.status_code, response.text))
            return None

    def request_transcription(self, transcript_url, api_key, audio_url):
        """Send the POST request to AssemblyAI API for transcription and return the transcript ID."""
        headers = {
            "Authorization": api_key,
            "Content-Type": "application/json"
        }
        data = {
            "audio_url": audio_url
        }

        try:
            print("Sending transcription request to AssemblyAI...")
            response = requests.post(transcript_url, headers=headers, json=data)

            if response.status_code == 200:
                transcript_data = response.json()
                return transcript_data['id']  # Return the transcript ID for polling
            else:
                print("Failed with status code {}: {}".format(response.status_code, response.text))
                return None

        except requests.RequestException as e:
            print("An error occurred while sending the request: {}".format(e))
            return None

    def poll_transcription_status(self, transcript_url, api_key, transcript_id):
        """Poll the transcription status until it is completed."""
        headers = {
            "Authorization": api_key,
            "Content-Type": "application/json"
        }

        while True:
            time.sleep(5)  # Wait for 5 seconds before polling
            response = requests.get("{}/{}".format(transcript_url, transcript_id), headers=headers)

            if response.status_code == 200:
                transcript_data = response.json()
                status = transcript_data['status']
                print("Current status:", status)

                if status == 'completed':
                    return transcript_data  # Return the completed transcript data
                elif status == 'failed':
                    print("Transcription failed.")
                    return None
            else:
                print("Failed to poll with status code {}: {}".format(response.status_code, response.text))
                return None]]>
                </content>
              </script>
              <Input name="onLoad" type="1" type_size="1" nature="0" inner="1" tooltip="Signal sent when diagram is loaded." id="1" />
              <Input name="onStart" type="0" type_size="1" nature="2" inner="0" tooltip="Box behavior starts when a signal is received on this input." id="2" />
              <Input name="onStop" type="1" type_size="1" nature="3" inner="0" tooltip="Box behavior stops when a signal is received on this input." id="3" />
              <Output name="onStopped" type="1" type_size="1" nature="1" inner="0" tooltip="Signal sent when box behavior is finished." id="4" />
              <Output name="output_1" type="0" type_size="1" nature="2" inner="0" tooltip="" id="5" />
            </Box>
            <Box name="filterProducts" id="3" localization="8" tooltip="" x="438" y="448">
              <bitmap>media/images/box/box-python-script.png</bitmap>
              <script language="4">
                <content>
                  <![CDATA[class MyClass(GeneratedClass):
    def _init_(self):
        GeneratedClass._init_(self)

    def onLoad(self):
        self.tts = ALProxy("ALTextToSpeech")  # Use TTS to speak messages

        # Product database for localization
        self.products_data = [
              {
                "name": "Rice",
                "price": 2.5,
                "ingredients": ["Rice"],
                "allergens": [],
                "location": {"aisle": 7, "section": "Left"}
              },
              {
                "name": "Almond Milk",
                "price": 3.0,
                "ingredients": ["Water", "Almonds"],
                "allergens": ["Nuts"],
                "location": {"aisle": 5, "section": "Right"}
              },
              # Additional product entries...
              {
                "name": "Chickpeas",
                "price": 1.6,
                "ingredients": ["Chickpeas", "Water", "Salt"],
                "allergens": [],
                "location": {"aisle": 8, "section": "Right"}
              }
        ]

    def onUnload(self):
        # Clean-up code here
        pass

    def onInput_onStart(self, product):
        # Example product received
        product = str(product)

        # Fetch location from database
        response = self.find_product_location(product)

        self.output_1(response)

        # Start of execution
        self.onStopped()  # Activate the output of the box

    def onInput_onStop(self):
        self.onUnload()  # Clean-up when the box is stopped
        self.onStopped()  # Activate the output of the box

    def find_product_location(self, product_name):
        # Convert product name to lowercase for case-insensitive matching
        product_name_lower = product_name.lower()

        # Iterate through the products to find a match
        for product in self.products_data:
            if product['name'].lower() == product_name_lower:

                # Extracting product details
                product_name_lower = product['name'].lower()  # Lowercase product name
                aisle = product['location']['aisle']  # Aisle number
                section = product['location']['section']  # Section name
                ingredients = ", ".join(product['ingredients'])  # Joining ingredients into a string

                # Handle allergens
                if product['allergens']:
                    allergens = ", ".join(product['allergens'])  # Joining allergens into a string
                else:
                    allergens = "No allergens listed"

                # Constructing the return string
                result = (
                    "The product " + product_name_lower +
                    " is located in aisle " + str(aisle) +
                    " and in section " + section +
                    ". Its ingredients are: " + ingredients +
                    ". Allergens: " + allergens + "."
                )

                return (result)

        return "Product not found."

    def onInput_input_1(self, p):
        # Receive the product name from input
        product_name = str(p)

        # Find the product location
        result = self.find_product_location(product_name)

        # Output the result
        self.tts = ALProxy("ALTextToSpeech")
        self.tts.say(result)

        self.onStopped()  # Activate the output of the box]]>
                </content>
              </script>
              <Input name="onLoad" type="1" type_size="1" nature="0" inner="1" tooltip="Signal sent when diagram is loaded." id="1" />
              <Input name="onStart" type="0" type_size="1" nature="2" inner="0" tooltip="Box behavior starts when a signal is received on this input." id="2" />
              <Input name="onStop" type="1" type_size="1" nature="3" inner="0" tooltip="Box behavior stops when a signal is received on this input." id="3" />
              <Input name="input_1" type="0" type_size="1" nature="1" inner="0" tooltip="" id="4" />
              <Output name="onStopped" type="1" type_size="1" nature="1" inner="0" tooltip="Signal sent when box behavior is finished." id="5" />
              <Output name="output_1" type="0" type_size="1" nature="2" inner="0" tooltip="" id="6" />
            </Box>
            <Box name="FullQuery" id="5" localization="8" tooltip="" x="557" y="447">
              <bitmap>media/images/box/box-python-script.png</bitmap>
              <script language="4">
                <content>
                  <![CDATA[import requests
from naoqi import ALProxy
import json

class MyClass(GeneratedClass):
    def __init__(self):
        # Correct __init__ method with double underscores
        GeneratedClass.__init__(self)
        self.conversation_history = []  # Initialize the conversation history as a list
        self.query = ""
    def onLoad(self):
        # Initialization code
        self.tts = ALProxy("ALTextToSpeech")  # Use TTS to speak messages

    def onUnload(self):
        # Clean-up code
        pass

    def onInput_input_1(self, full_query):
#        self.tts.say("received query is " + str(full_query))

        # Concatenate to self.query instead of append
        self.query += "customers query is " + str(full_query)
#        self.tts.say(self.query)

        self.onStopped()

    def onInput_onStart(self, p):
        product = str(p)  # Get the user's input
        self.query += " and product detail is " + product

        # Add the user's message to the conversation history
        self.conversation_history.append({
            "role": "user",
            "content": "give me short and concise response of customer query. " + self.query
        })

        # Example message to demonstrate the working list
#        self.tts.say("User input has been added to history")

        try:
            # Define the URL of the Ollama server API
            url = 'http://192.168.76.153:11434/api/chat'

            # Prepare the payload for Ollama including the conversation history
            payload = {
                "model": "llama3.1",
                "messages": self.conversation_history  # Send the conversation history
            }

            self.tts.say("Please wait while i am processing for second round to ollama.")
#            self.tts.say("Respond according to question." + self.query)


            # Send the request to the Ollama server
            response = requests.post(url, json=payload)

            if response.status_code == 200:
                full_response = ""
                # Parse the JSON response
                raw_data = response.text.strip().splitlines()
                for item in raw_data:
                    parsed_item = json.loads(item)
                    message = parsed_item.get('message', {})
                    response_value = message.get('content', "")
                    full_response += response_value  # Accumulate the text

                self.tts.say(str(full_response.strip()))  # Speak the accumulated text

                self.conversation_history.append({
                    "role": "assistant",
                    "content": full_response.strip()
                })
                # Activate the output of the box


            else:
                self.tts.say("Failed to fetch a response. Status code: " + str(response.status_code))

        except Exception as e:
            # Catching any exception and reporting it using TTS
            error_message = "Exception: {0}".format(str(e))
            print(error_message)  # Log the error message for debugging
            self.tts.say(error_message)


        self.onStopped()

    def onInput_onStop(self, full_query):

        self.onUnload()  # Clean up when the box is stopped
        self.onStopped()  # Activate the output of the box]]>
                </content>
              </script>
              <Input name="onLoad" type="1" type_size="1" nature="0" inner="1" tooltip="Signal sent when diagram is loaded." id="1" />
              <Input name="onStart" type="0" type_size="1" nature="2" inner="0" tooltip="Box behavior starts when a signal is received on this input." id="2" />
              <Input name="onStop" type="1" type_size="1" nature="3" inner="0" tooltip="Box behavior stops when a signal is received on this input." id="3" />
              <Input name="input_1" type="0" type_size="1" nature="1" inner="0" tooltip="" id="4" />
              <Output name="onStopped" type="1" type_size="1" nature="1" inner="0" tooltip="Signal sent when box behavior is finished." id="5" />
            </Box>
            <Link inputowner="7" indexofinput="2" outputowner="1" indexofoutput="5" />
            <Link inputowner="2" indexofinput="2" outputowner="7" indexofoutput="5" />
            <Link inputowner="1" indexofinput="2" outputowner="2" indexofoutput="4" />
            <Link inputowner="9" indexofinput="2" outputowner="8" indexofoutput="5" />
            <Link inputowner="4" indexofinput="2" outputowner="9" indexofoutput="5" />
            <Link inputowner="5" indexofinput="4" outputowner="4" indexofoutput="6" />
            <Link inputowner="5" indexofinput="2" outputowner="3" indexofoutput="6" />
            <Link inputowner="3" indexofinput="2" outputowner="4" indexofoutput="5" />
            <Link inputowner="1" indexofinput="2" outputowner="0" indexofoutput="2" />
          </Diagram>
        </BehaviorKeyframe>
      </BehaviorLayer>
    </Timeline>
  </Box>
</ChoregrapheProject>
